{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Generator\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from nnx import tinygrad\n",
    "from nnx.tinygrad.activations import ReLU, Softmax\n",
    "from nnx.tinygrad.initialisation import xavier_uniform\n",
    "from nnx.tinygrad.layers import Dropout, LayerNorm, Linear, Reshape, Sequential\n",
    "from nnx.tinygrad.loss import Tensor, cross_entropy_loss\n",
    "from nnx.tinygrad.optimizer import AdamW\n",
    "\n",
    "seed = 3\n",
    "\n",
    "tinygrad.rng = np.random.default_rng(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "----\n",
    "This notebook demonstrates the TinyGrad framework using the Fashion MNIST dataset. While the focus is on showing the neural network architecture implementation, the torchvision dataset loader is used for convenience. Fashion MNIST serves as a good example since the input resolution is small what eases training time for a CPU only implementation and still has 10 distinct non-trivial classes which need to be classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = torchvision.datasets.FashionMNIST(\n",
    "    \".\",\n",
    "    train=True,\n",
    ")  # for initial download set download=True\n",
    "validation_data = torchvision.datasets.FashionMNIST(\".\", train=False)\n",
    "\n",
    "\n",
    "def create_mnist_batch_loader(dataset, batch_size=64) -> Generator:\n",
    "    \"\"\"Create a batch loader for Fashion MNIST dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset: The PyTorch dataset (e.g., FashionMNIST)\n",
    "        batch_size: Number of samples per batch\n",
    "\n",
    "    Yields:\n",
    "        Generator that yields batches of (images, one-hot labels).\n",
    "\n",
    "    \"\"\"\n",
    "    dataset_size = len(dataset)\n",
    "    indices = np.arange(dataset_size)\n",
    "    num_batches = int(dataset_size // batch_size)\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        # Get indices for this batch\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, dataset_size)\n",
    "        batch_indices = indices[start_idx:end_idx]\n",
    "\n",
    "        # Initialize batch arrays\n",
    "        actual_batch_size = len(batch_indices)\n",
    "        batch_images = np.zeros((actual_batch_size, 28, 28, 1), dtype=np.float32)\n",
    "        batch_labels = np.zeros((actual_batch_size, 10), dtype=np.float32)\n",
    "\n",
    "        for i, idx in enumerate(batch_indices):\n",
    "            image, label = dataset[idx]\n",
    "\n",
    "            # Convert PIL image to numpy array, normalize to [0,1]\n",
    "            img_array = np.array(image, dtype=np.float32)[:, :, None] / 255.0\n",
    "\n",
    "            batch_images[i] = img_array\n",
    "            batch_labels[i, label] = 1.0  # One-hot encoding\n",
    "\n",
    "        images_tensor = Tensor(batch_images)\n",
    "        labels_tensor = Tensor(batch_labels)\n",
    "\n",
    "        yield images_tensor, labels_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "--------\n",
    "Below is the definition of the neural network architecture and optimizer. The model implementation demonstrates TinyGrad's PyTorch-inspired API design. A multi-layer perceptron (MLP) architecture is used rather than convolutional layers in the initial layers for feature extraction, as the current `Conv2D` implementation prioritizes educational clarity over performance optimization.\n",
    "\n",
    "The network architecture includes several fully-connected layers with dropout regularization and ReLU activations, concluding with a softmax layer for classification. Batch size and epochs were chosen by heart and were not the product of a grid/random search for optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "batch_size = 512\n",
    "epochs = 10\n",
    "\n",
    "network = Sequential(\n",
    "    Reshape((batch_size, 784)),\n",
    "    Linear(784, 784, initialiser=xavier_uniform, bias=True),\n",
    "    LayerNorm(784),\n",
    "    Dropout(0.1),\n",
    "    ReLU(),\n",
    "    Linear(784, 512, initialiser=xavier_uniform, bias=True),\n",
    "    LayerNorm(512),\n",
    "    Dropout(0.1),\n",
    "    ReLU(),\n",
    "    Linear(512, 256, initialiser=xavier_uniform, bias=True),\n",
    "    LayerNorm(256),\n",
    "    Dropout(0.1),\n",
    "    ReLU(),\n",
    "    Linear(256, 64, initialiser=xavier_uniform, bias=True),\n",
    "    LayerNorm(64),\n",
    "    Dropout(0.1),\n",
    "    ReLU(),\n",
    "    Linear(64, num_classes, initialiser=xavier_uniform, bias=True),\n",
    "    LayerNorm(num_classes),\n",
    "    Softmax(),\n",
    ")\n",
    "\n",
    "optimizer = AdamW(network.parameters, lr=1e-3, weight_decay=1e-2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "---------------\n",
    "To monitor training progress multiple visualization functions were implemented. These help us track:\n",
    "- Gradient flow through the network\n",
    "- Validation accuracy over time\n",
    "- Model predictions on sample data\n",
    "\n",
    "While these visualization utilities are not core to the TinyGrad implementation, they provide valuable insights into model training dynamics and performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import plot_gradient_flow, plot_model_predictions, plot_validation_accuracy\n",
    "\n",
    "\n",
    "def extract_gradient_stats(model: Sequential, gradient_history: list) -> None:\n",
    "    \"\"\"Extract gradients for visualisation purposes.\"\"\"\n",
    "    grad_stats = {}\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if hasattr(layer, \"parameters\"):\n",
    "            for j, param in enumerate(layer.parameters):\n",
    "                if param.grad is not None:\n",
    "                    layer_name = f\"Layer {i}-Param {j}\"\n",
    "                    grad_stats[layer_name] = np.mean(np.abs(param.grad))\n",
    "    gradient_history.append(grad_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "---------\n",
    "In this section, we train our model on the Fashion MNIST dataset, tracking metrics throughout the process. The training loop demonstrates TinyGrad's end-to-end capabilities, from forward and backward passes to optimization and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accuracies = []\n",
    "gradient_history = []\n",
    "\n",
    "for epoch_idx in range(epochs):\n",
    "    loss_per_epoch = []\n",
    "    batch_pbar = tqdm(\n",
    "        create_mnist_batch_loader(training_data, batch_size=batch_size),\n",
    "        desc=\"Starting training...\",\n",
    "    )\n",
    "\n",
    "    for idx, [images, targets] in enumerate(batch_pbar):\n",
    "        output = network(images)\n",
    "        loss = cross_entropy_loss(output, targets)\n",
    "\n",
    "        batch_pbar.set_description(f\"Epoch {epoch_idx + 1}/{epochs} | Batch {idx}\")\n",
    "        batch_pbar.set_postfix({\"loss\": f\"{loss.data:.4f}\"})\n",
    "        loss_per_epoch.append(loss.data)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        extract_gradient_stats(network, gradient_history)\n",
    "\n",
    "    losses.append(loss_per_epoch)\n",
    "\n",
    "    # Valiation logic\n",
    "    val_pbar = tqdm(\n",
    "        create_mnist_batch_loader(validation_data, batch_size=batch_size),\n",
    "        desc=\"Starting validation pass...\",\n",
    "    )\n",
    "\n",
    "    tmp = []\n",
    "    for idx, [images, targets] in enumerate(val_pbar):\n",
    "        output = network(images)\n",
    "\n",
    "        predicted_classes = np.argmax(output.data, axis=1)\n",
    "        target_classes = np.argmax(targets.data, axis=1)\n",
    "        result = predicted_classes == target_classes\n",
    "\n",
    "        tmp.extend(result.tolist())\n",
    "        batch_accuracy = result.mean()\n",
    "\n",
    "        val_pbar.set_description(f\"Iteration {idx}\")\n",
    "        batch_pbar.set_postfix({\"accuracy\": f\"{batch_accuracy}\"})\n",
    "    accuracies.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.eval()\n",
    "\n",
    "fig_gradients = plot_gradient_flow(gradient_history)\n",
    "fig_gradients.savefig(\"images/gradient_flow.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "fig = plot_validation_accuracy(\n",
    "    [sum(acc) / len(acc) for acc in accuracies],\n",
    "    [sum(loss) / len(loss) for loss in losses],\n",
    ")\n",
    "fig.savefig(\"images/accuracy_plot.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "val_images, val_labels = next(\n",
    "    iter(create_mnist_batch_loader(validation_data, batch_size=batch_size)),\n",
    ")\n",
    "predictions = network(val_images)\n",
    "class_names = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "fig = plot_model_predictions(\n",
    "    val_images.data,\n",
    "    val_labels.data,\n",
    "    predictions.data,\n",
    "    class_names,\n",
    "    num_samples=36,\n",
    ")\n",
    "fig.savefig(\"images/prediction_showcase.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
