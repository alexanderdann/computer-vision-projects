{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "seed = 3\n",
    "\n",
    "rng = np.random.default_rng(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_uniform(\n",
    "    fan_in: int,\n",
    "    fan_out: int,\n",
    "    size: list[int],\n",
    "    gain: float = 1,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Initialize the weights using the Xavier Uniform.\n",
    "\n",
    "    Details can be found in  https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf.\n",
    "\n",
    "    Args:\n",
    "        fan_in: the amount of input neurons.\n",
    "        fan_out: the amout of output neurons.\n",
    "        size: dimensions of the data to initialise.\n",
    "        gain: optional factor which can also be applied in PyTorch.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the weights and bias initialized as per Xavier Uniform.\n",
    "\n",
    "    \"\"\"\n",
    "    limit = gain * np.sqrt(6 / (fan_in + fan_out))\n",
    "    weights = rng.uniform(-limit, limit, size=size)\n",
    "    bias = np.zeros((fan_out, 1))\n",
    "\n",
    "    return weights, bias\n",
    "\n",
    "\n",
    "def sanity_checks(func: callable) -> callable:\n",
    "    \"\"\"Check the input to have correct properties.\n",
    "\n",
    "    Args:\n",
    "        func: a callable used to evaluate np arrays.\n",
    "\n",
    "    Returns:\n",
    "        The wrapper function.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def _wrapper(self, inputs: Tensor) -> Tensor:\n",
    "        expected_dim = 4\n",
    "        if inputs.data.ndim != expected_dim:\n",
    "            msg = f\"Data needs to have ndim of 4, got {inputs.data.ndim}\"\n",
    "            raise RuntimeError(msg)\n",
    "\n",
    "        _, _, _, num_channels = inputs.data.shape\n",
    "\n",
    "        if num_channels != self._in_dim:\n",
    "            msg = (\n",
    "                \"Expected number of channels\",\n",
    "                f\"to be {self._in_dim} and not {num_channels}.\",\n",
    "            )\n",
    "            raise RuntimeError(msg)\n",
    "\n",
    "        return func(self, inputs)\n",
    "\n",
    "    return _wrapper\n",
    "\n",
    "\n",
    "class Tensor:\n",
    "    \"\"\"Resembles a data structure to carry the data, gradients and topology.\n",
    "\n",
    "    Inspired by Andrej Karpathy https://github.com/karpathy/micrograd\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: np.ndarray, *, requires_grad: bool = False) -> None:\n",
    "        \"\"\"C'tor of Tensor.\n",
    "\n",
    "        Args:\n",
    "            data: contents associated with the tensor.\n",
    "            requires_grad: whether we want to store gradients infomation.\n",
    "\n",
    "        \"\"\"\n",
    "        self._data = data\n",
    "        self._requires_grad = requires_grad\n",
    "        self._grad: np.ndarray | None = None\n",
    "        self._backward = (\n",
    "            lambda: None\n",
    "        )  # A function that computes the gradient propagation.\n",
    "        self._prev = set()  # A set of Tensors that were used to compute this one.\n",
    "\n",
    "    @property\n",
    "    def data(self) -> np.ndarray:\n",
    "        \"\"\"Exposure of internal data.\"\"\"\n",
    "        return self._data\n",
    "\n",
    "    @property\n",
    "    def requires_grad(self) -> bool:\n",
    "        \"\"\"Read only property of whether the Tensor requires a grad.\"\"\"\n",
    "        return self._requires_grad\n",
    "\n",
    "    @property\n",
    "    def grad(self) -> np.ndarray | None:\n",
    "        \"\"\"Return the gradients.\"\"\"\n",
    "        return self._grad\n",
    "\n",
    "    @grad.setter\n",
    "    def grad(self, value: np.ndarray | None) -> None:\n",
    "        self._grad = value\n",
    "\n",
    "    def register_backward(self, func: callable) -> None:\n",
    "        \"\"\"Register the closure to compute backward pass.\"\"\"\n",
    "        self._backward = func\n",
    "\n",
    "    def backward(self, grad: np.ndarray | None = None) -> None:\n",
    "        \"\"\"Compute the backward pass.\"\"\"\n",
    "        if grad is None:\n",
    "            grad = np.ones_like(self._data)\n",
    "        # Accumulate gradients.\n",
    "        self._grad = grad if self._grad is None else self._grad + grad\n",
    "        self._backward()\n",
    "        for t in self._prev:\n",
    "            t.backward()\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"Meta class for all layers.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"C'tor of Layer.\"\"\"\n",
    "        self._parameters: list[Tensor] = []\n",
    "\n",
    "    @property\n",
    "    def parameters(self) -> list[Tensor]:\n",
    "        \"\"\"Returns the parameters of the layer.\"\"\"\n",
    "        return self._parameters\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Logic of the forward pass.\"\"\"\n",
    "\n",
    "    @sanity_checks\n",
    "    def __call__(self, inputs: Tensor) -> Tensor:\n",
    "        \"\"\"Wrap the forward call.\n",
    "\n",
    "        Returns:\n",
    "            The result after the forward pass.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.forward(inputs)\n",
    "\n",
    "\n",
    "class Conv2D(Layer):\n",
    "    \"\"\"Resembles a 2D Convolution.\"\"\"\n",
    "\n",
    "    def __init__(  # noqa: PLR0913\n",
    "        self,\n",
    "        kernel_size: tuple[int, int],\n",
    "        in_dim: int,\n",
    "        out_dim: int,\n",
    "        padding: int = 0,\n",
    "        stride: int = 1,\n",
    "        *,\n",
    "        bias: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"C'tor of Conv2D.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: for an invalid kernel size.\n",
    "\n",
    "        Args:\n",
    "            kernel_size: spatial dimension of the kernel.\n",
    "            in_dim: count of input neurons.\n",
    "            out_dim: count of output neurons.\n",
    "            bias: whether we want to use the bias term.\n",
    "            padding: constant padding to the sides in pixel.\n",
    "            stride: step size for the convolution operation.\n",
    "\n",
    "        \"\"\"\n",
    "        self._in_dim = in_dim\n",
    "        self._out_dim = out_dim\n",
    "        self._padding = padding\n",
    "        self._stride = stride\n",
    "\n",
    "        expected_size = 2\n",
    "        if len(kernel_size) != expected_size:\n",
    "            msg = (\n",
    "                \"Kernel size is expected to be tuple of length two\",\n",
    "                f\", not {len(kernel_size)}\",\n",
    "            )\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        weights, bias_ = xavier_uniform(\n",
    "            in_dim,\n",
    "            out_dim,\n",
    "            size=(out_dim, *kernel_size, in_dim),\n",
    "        )\n",
    "        self._weights = Tensor(weights, requires_grad=True)\n",
    "        self._bias = Tensor(bias_, requires_grad=True) if bias else None\n",
    "\n",
    "    @staticmethod\n",
    "    def _conv2d_forward(\n",
    "        inputs: np.ndarray,\n",
    "        weights: np.ndarray,\n",
    "        bias: np.ndarray | None,\n",
    "        padding: int,\n",
    "        stride: int,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Calculate spatial convolution on inputs.\n",
    "\n",
    "        Returns:\n",
    "            Transformation applied to the image.\n",
    "\n",
    "        \"\"\"\n",
    "        num_samples, height, width, _ = inputs.shape\n",
    "        output_filters, kernel_height, kernel_width, _ = weights.shape\n",
    "\n",
    "        out_height = (height + 2 * padding - kernel_height) // stride + 1\n",
    "        out_width = (width + 2 * padding - kernel_width) // stride + 1\n",
    "\n",
    "        if padding:\n",
    "            inputs = np.pad(\n",
    "                inputs,\n",
    "                ((0, 0), (padding, padding), (padding, padding), (0, 0)),\n",
    "                mode=\"constant\",\n",
    "            )\n",
    "\n",
    "        # Initialize the output tensor.\n",
    "        outputs = np.zeros((num_samples, out_height, out_width, output_filters))\n",
    "\n",
    "        for sample_id in range(num_samples):\n",
    "            for pixel_h in range(out_height):\n",
    "                for pixel_w in range(out_width):\n",
    "                    for filter_id in range(output_filters):\n",
    "                        start_h = pixel_h * stride\n",
    "                        start_w = pixel_w * stride\n",
    "\n",
    "                        patch = inputs[\n",
    "                            sample_id,\n",
    "                            start_h : start_h + kernel_height,\n",
    "                            start_w : start_w + kernel_width,\n",
    "                            :,\n",
    "                        ]\n",
    "\n",
    "                        results = np.sum(patch * weights[filter_id, :, :, :])\n",
    "\n",
    "                        if bias is not None:\n",
    "                            results += bias[filter_id].item()\n",
    "\n",
    "                        outputs[sample_id, pixel_h, pixel_w, filter_id] = results\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    def _conv2d_backward(\n",
    "        inputs: np.ndarray,\n",
    "        weights: np.ndarray,\n",
    "        bias: np.ndarray | None,\n",
    "        grad_output: np.ndarray,\n",
    "        padding: int,\n",
    "        stride: int,\n",
    "    ) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        num_samples, in_height, in_width, in_channels = inputs.shape\n",
    "        out_channels, kernel_height, kernel_width, _ = weights.shape\n",
    "        _, out_height, out_width, _ = grad_output.shape\n",
    "\n",
    "        # Initialize gradients\n",
    "        dx = np.zeros_like(inputs)\n",
    "        dweight = np.zeros_like(weights)\n",
    "        dbias = np.zeros_like(bias) if bias is not None else None\n",
    "\n",
    "        if padding:\n",
    "            inputs = np.pad(\n",
    "                inputs,\n",
    "                ((0, 0), (padding, padding), (padding, padding), (0, 0)),\n",
    "                mode=\"constant\",\n",
    "            )\n",
    "\n",
    "        # Gradients with respect to bias\n",
    "        # This is basically just a spatial summation of all samples\n",
    "        if bias is not None:\n",
    "            dbias = np.sum(grad_output, axis=(0, 1, 2)).reshape(out_channels, 1)\n",
    "\n",
    "        # Gradients with respect to weights\n",
    "        # Fairly simple since a single weight only contributes along a matching index\n",
    "        # Given y(i, j) = ∑_{p'}∑_{q'} x(i+p, j+q)*w(p,q) and L(y(i, j))\n",
    "        # -> Derivative for a single w(m, n) only valid for x(i+m, j+n)\n",
    "        # Hence this is nothing else than elemntwise multiplication\n",
    "        for sample_id in range(num_samples):\n",
    "            for h_out in range(out_height):\n",
    "                for w_out in range(out_width):\n",
    "                    start_h = h_out * stride\n",
    "                    start_w = w_out * stride\n",
    "\n",
    "                    input_patch = inputs[\n",
    "                        sample_id,\n",
    "                        start_h : start_h + kernel_height,\n",
    "                        start_w : start_w + kernel_width,\n",
    "                        :,\n",
    "                    ]\n",
    "\n",
    "                    for c_out in range(out_channels):\n",
    "                        dweight[c_out] += (\n",
    "                            input_patch * grad_output[sample_id, h_out, w_out, c_out]\n",
    "                        )\n",
    "\n",
    "        # Gradients with respect to weights\n",
    "        # Since the formula from above contains a `x(i+p, j+q)`, the only addition is\n",
    "        # that we need to take care of the additions. These just lead to a flipped\n",
    "        # kernel in the end.\n",
    "        pad_h = kernel_height - 1\n",
    "        pad_w = kernel_width - 1\n",
    "        grad_output_padded = np.pad(\n",
    "            grad_output,\n",
    "            ((0, 0), (pad_h, pad_h), (pad_w, pad_w), (0, 0)),\n",
    "            mode=\"constant\",\n",
    "        )\n",
    "\n",
    "        # Rotate the weights (equal to flipping along both spatial dimensions)\n",
    "        # But also take into account that the in_channels and out_channels changed\n",
    "        flipped_weights = np.flip(np.flip(weights, axis=1), axis=2)\n",
    "        flipped_weights = np.transpose(flipped_weights, (3, 1, 2, 0))\n",
    "\n",
    "        for sample_id in range(num_samples):\n",
    "            for h_in in range(in_height):\n",
    "                for w_in in range(in_width):\n",
    "                    for c_in in range(in_channels):\n",
    "                        # For each input element, convolve the corresponding region\n",
    "                        # in grad_output with the flipped weights\n",
    "\n",
    "                        # Extract the patch from padded grad_output\n",
    "                        grad_patch = grad_output_padded[\n",
    "                            sample_id,\n",
    "                            h_in : h_in + kernel_height,\n",
    "                            w_in : w_in + kernel_width,\n",
    "                            :,\n",
    "                        ]\n",
    "\n",
    "                        # Dot product with the flipped weights for this input channel\n",
    "                        dx[sample_id, h_in, w_in, c_in] = np.sum(\n",
    "                            grad_patch * flipped_weights[c_in],\n",
    "                        )\n",
    "\n",
    "        # Remove padding if it was applied\n",
    "        if padding:\n",
    "            dx = dx[:, padding:-padding, padding:-padding, :]\n",
    "\n",
    "        return dx, dweight, dbias\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        \"\"\"Compute the transformation given the inputs.\n",
    "\n",
    "        Args:\n",
    "            inputs: Tensor which needs to be transformed.\n",
    "\n",
    "        Returns:\n",
    "            Transformed Tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        outputs: np.ndarray = self._conv2d_forward(\n",
    "            inputs=inputs.data,\n",
    "            weights=self._weights.data,\n",
    "            bias=self._bias.data if self._bias is not None else None,\n",
    "            padding=self._padding,\n",
    "            stride=self._stride,\n",
    "        )\n",
    "\n",
    "        outputs: Tensor = Tensor(\n",
    "            outputs, requires_grad=inputs.requires_grad or self._weights.requires_grad\n",
    "        )\n",
    "\n",
    "        def _backward() -> None:\n",
    "            dx, dweight, dbias = self._conv2d_backward(\n",
    "                inputs.data,\n",
    "                self._weights.data,\n",
    "                self._bias.data if self._bias is not None else None,\n",
    "                outputs.data,\n",
    "                self._stride,\n",
    "                self._padding,\n",
    "            )\n",
    "            if inputs.requires_grad:\n",
    "                inputs.grad = dx if inputs.grad is None else inputs.grad + dx\n",
    "            if self._weights.requires_grad:\n",
    "                self._weights.grad = (\n",
    "                    dweight\n",
    "                    if self._weights.grad is None\n",
    "                    else self._weights.grad + dweight\n",
    "                )\n",
    "            if self._bias is not None and self._bias.requires_grad:\n",
    "                self._bias.grad = (\n",
    "                    dbias if self._bias.grad is None else self._bias.grad + dbias\n",
    "                )\n",
    "\n",
    "        outputs.register_backward(_backward)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Tensor(rng.random((1, 32, 32, 3)), requires_grad=True)\n",
    "layer_1 = Conv2D((3, 3), 3, 64)\n",
    "layer_2 = Conv2D((3, 3), 64, 128)\n",
    "layer_3 = Conv2D((3, 3), 128, 3)\n",
    "\n",
    "\n",
    "outputs = layer_3(layer_2(layer_1(inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.backward(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
