{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Callable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nnx.data_structures.union_find import BetterUnionFind, NaiveUnionFind\n",
    "\n",
    "seed = 3\n",
    "\n",
    "rng = np.random.default_rng(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "-----\n",
    "Initially we start with a comparison of the algorithms for Union-Find. As a starting point we implemnt a naive version is also know as Quick-Find where in worst case we can have $O(N^{2})$ runtime. Afterwards, we optimize it by adding a weighting to reduce the unnecessary risk of having subgraphs with high depth and also using path reduction. Beneficial about it that we can reduce the worst case scenario to $O(N+M \\times ln^{*}(N))$.\n",
    "\n",
    "![union_find_complexity.png](images/union_find/union_find_complexity.png)\n",
    "\n",
    "Finally, we then move to an actual problem of appliance for this data structure. Namely, estimating percolation threshold using Monte Carlo methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_operations(num_elements: int) -> list[tuple[int, int]]:\n",
    "    \"\"\"Generate a sequence of random merge operations for Union Find testing.\n",
    "\n",
    "    Args:\n",
    "        num_elements: Number of elements for union find.\n",
    "\n",
    "    Returns:\n",
    "        List of (node1, node2) tuples representing merge operations.\n",
    "\n",
    "    \"\"\"\n",
    "    operations = []\n",
    "    for _ in range(num_elements):\n",
    "        # Generate random pair of nodes to merge\n",
    "        a = rng.integers(0, num_elements - 1)\n",
    "        b = rng.integers(0, num_elements - 1)\n",
    "\n",
    "        operations.append((a, b))\n",
    "\n",
    "    return operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Differences in Timing\n",
    "------\n",
    "For the same configuration we can sample different operations and calculate the time it needed to compute these. Since we talk of asymptotic runtimes, we need to sample different constallations for the same hyperparameters for reduce the effect of noise. In this case the `num_elements`. When using a large `num_trials` when can generate many runs for the same configuration of hyperparameters and come close to the asymptotical complexity. \n",
    "\n",
    "To not overcomplicate matters, we run it for `num_trials=50` and report the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_algo(\n",
    "    algo: NaiveUnionFind | BetterUnionFind,\n",
    "    operations: list,\n",
    ") -> float:\n",
    "    \"\"\"Time the computation of the algorithm for a given set of operations.\n",
    "\n",
    "    Args:\n",
    "        algo: the algorithm to benchmark\n",
    "        operations: set of operations to perform\n",
    "\n",
    "    Returns:\n",
    "        The time needed to perform all operations\n",
    "\n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "    for operation in operations:\n",
    "        algo.union(*operation)\n",
    "\n",
    "    return time.perf_counter() - start\n",
    "\n",
    "\n",
    "times_naive = []\n",
    "times_better = []\n",
    "\n",
    "sizes = range(10, int(2.5e3), 10)\n",
    "num_trials = 50  # reduce this to get quicker but noisier results\n",
    "\n",
    "for num_elements in tqdm(sizes):\n",
    "    sampling_naive = []\n",
    "    sampling_better = []\n",
    "    for _ in range(num_trials):\n",
    "        operations = generate_operations(num_elements=num_elements)\n",
    "\n",
    "        sampling_naive.append(time_algo(NaiveUnionFind(num_elements), operations))\n",
    "        sampling_better.append(time_algo(BetterUnionFind(num_elements), operations))\n",
    "\n",
    "    # taking quantiles instead of average to have reduce effect of outliers\n",
    "    times_naive.append(np.quantile(sampling_naive, 0.5))\n",
    "    times_better.append(np.quantile(sampling_better, 0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _log_star(n: int) -> float:\n",
    "    if n <= 1:\n",
    "        return 0\n",
    "    count = 0\n",
    "    while n > 1:\n",
    "        n = np.log(n)\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def theoretical_complexity(\n",
    "    n: int,\n",
    "    complexity_fn: Callable[[int], float],\n",
    "    scale_factor: float,\n",
    ") -> float:\n",
    "    \"\"\"Calculate theoretical complexity based on input size.\n",
    "\n",
    "    Args:\n",
    "        n: Input size\n",
    "        complexity_fn: Function that calculates the complexity (e.g., n^2)\n",
    "        scale_factor: Scaling factor to match with actual times\n",
    "\n",
    "    Returns:\n",
    "        Theoretical time\n",
    "\n",
    "    \"\"\"\n",
    "    return complexity_fn(n) * scale_factor\n",
    "\n",
    "\n",
    "def plot_union_find_complexity(\n",
    "    sizes: list[int],\n",
    "    times_naive: list[float],\n",
    "    times_better: list[float],\n",
    ") -> None:\n",
    "    \"\"\"Create a plot comparing actual times with theoretical complexity.\n",
    "\n",
    "    Args:\n",
    "        sizes: List of problem sizes\n",
    "        times_naive: Execution times for naive algorithm\n",
    "        times_better: Execution times for better algorithm\n",
    "\n",
    "    \"\"\"\n",
    "    sizes_array = np.array(sizes)\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    ax.plot(sizes, times_naive, \"b-\", label=\"Naive Union Find (Actual)\", linewidth=2)\n",
    "    ax.plot(\n",
    "        sizes,\n",
    "        times_better,\n",
    "        \"g-\",\n",
    "        label=\"Weighted Union Find with Path Compression (Actual)\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "\n",
    "    def _naive_complexity_fn(n: int) -> float:\n",
    "        return n**2\n",
    "\n",
    "    def _better_complexity_fn(n: int) -> float:\n",
    "        return n + n * _log_star(n)\n",
    "\n",
    "    naive_complexity_values = np.array([_naive_complexity_fn(n) for n in sizes_array])\n",
    "    better_complexity_values = np.array([_better_complexity_fn(n) for n in sizes_array])\n",
    "\n",
    "    # For naive: O(n**2)\n",
    "    naive_scale = (\n",
    "        np.mean(np.array(times_naive) / naive_complexity_values)\n",
    "        if len(times_naive) > 0\n",
    "        else 1e-10\n",
    "    )\n",
    "\n",
    "    # For better: O(n + m log* n)\n",
    "    better_scale = (\n",
    "        np.mean(np.array(times_better) / better_complexity_values)\n",
    "        if len(times_better) > 0\n",
    "        else 1e-10\n",
    "    )\n",
    "\n",
    "    # Generate theoretical curves with the scaling factors\n",
    "    theo_naive = naive_complexity_values * naive_scale\n",
    "    theo_better = better_complexity_values * better_scale\n",
    "\n",
    "    ax.plot(\n",
    "        sizes,\n",
    "        theo_naive,\n",
    "        \"b--\",\n",
    "        label=\"Naive Union Find (Theoretical O(n²))\",\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    ax.plot(\n",
    "        sizes,\n",
    "        theo_better,\n",
    "        \"g--\",\n",
    "        label=\"Weighted UF with Path Compression (Theoretical O(n + m log* n))\",\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "    if len(times_naive) > 0 and len(times_better) > 0:\n",
    "        speedup = np.array(times_naive) / np.array(times_better)\n",
    "        max_speedup_idx = np.argmax(speedup)\n",
    "        max_speedup = speedup[max_speedup_idx]\n",
    "        max_speedup_size = sizes[max_speedup_idx]\n",
    "\n",
    "        # Add annotation for maximum speedup\n",
    "        ax.annotate(\n",
    "            f\"Max Speedup: {max_speedup:.2f}x at n={max_speedup_size}\",\n",
    "            xy=(max_speedup_size, times_naive[max_speedup_idx]),\n",
    "            xytext=(max_speedup_size + 300, times_naive[max_speedup_idx] * 1.2),\n",
    "            arrowprops={\n",
    "                \"facecolor\": \"black\",\n",
    "                \"shrink\": 0.05,\n",
    "                \"width\": 1.5,\n",
    "                \"headwidth\": 8,\n",
    "            },\n",
    "            fontsize=10,\n",
    "        )\n",
    "\n",
    "    # Add log scale for y-axis to better visualize differences\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "    ax.grid(visible=True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Add labels and title\n",
    "    ax.set_xlabel(\"Number of Elements (n)\", fontsize=12)\n",
    "    ax.set_ylabel(\"Time (seconds)\", fontsize=12)\n",
    "    ax.set_title(\"Union Find Algorithms: Time Complexity Comparison\", fontsize=14)\n",
    "\n",
    "    ax.legend(fontsize=10)\n",
    "\n",
    "    # Format y-axis to show actual time values\n",
    "    ax.yaxis.set_major_formatter(ScalarFormatter())\n",
    "\n",
    "    # Add a secondary y-axis for showing the speedup\n",
    "    if len(times_naive) > 0 and len(times_better) > 0:\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(sizes, speedup, \"r-\", label=\"Speedup Factor\", alpha=0.6)\n",
    "        ax2.set_ylabel(\"Speedup Factor (Naive / Better)\", color=\"r\", fontsize=12)\n",
    "        ax2.tick_params(axis=\"y\", labelcolor=\"r\")\n",
    "        ax2.legend(fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"union_find_complexity.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_union_find_complexity(sizes, times_naive, times_better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Simulation for Percolation\n",
    "------\n",
    "## Overview\n",
    "Percolation theory studies how connectivity emerges in random networks. This simulation implements site percolation on a 2D square using Monte Carlo methods.\n",
    "\n",
    "## Methodology\n",
    "The simulation works by:\n",
    "1. Starting with an empty grid where all nodes are inactive\n",
    "2. Randomly opening nodes one by one\n",
    "3. Using a Union-Find data structure to efficiently track connected clusters\n",
    "4. Determining the critical threshold when a connected path forms from top to bottom\n",
    "5. Repeating this process multiple times to obtain statistical estimates\n",
    "\n",
    "## Percolation Threshold\n",
    "The percolation threshold is the ratio (active to inactive nodes) at which a connected path appears. In the implementation this is measured as the ratio of active nodes to total nodes at the moment of percolation. Increasing the grid size leads to a sharper transistion and ultimately for an infinite 2D square, the theoretical threshold is approximately 0.593. \n",
    "\n",
    "![percolation_probabilities](images/union_find/percolation_probabilities.png)\n",
    "\n",
    "## Implementation Notes\n",
    "This code uses a weighted quick-union algorithm with path compression for efficient cluster identification. Virtual top and bottom nodes are used to simplify the detection of a percolating path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_index(row: int, col: int, grid_size: int) -> int:\n",
    "    \"\"\"Translate row and col from grid to flattened.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: in the case where range exceeds the allowed range.\n",
    "\n",
    "    Returns:\n",
    "        Translated index.\n",
    "\n",
    "    \"\"\"\n",
    "    if (abs(row) > grid_size) or (abs(col) > grid_size):\n",
    "        msg = f\"{row} and {col} exceed the allowed range of [0, {grid_size - 1}].\"\n",
    "        raise RuntimeError(msg)\n",
    "    row = row if row >= 0 else grid_size + row\n",
    "    col = col if col >= 0 else grid_size + col\n",
    "\n",
    "    return grid_size * row + col\n",
    "\n",
    "\n",
    "def simulate_experiment(grid_size: int) -> float:\n",
    "    \"\"\"Simulate a single trial.\n",
    "\n",
    "    Returns:\n",
    "        The percolation threshold which is the ratio of black and white.\n",
    "\n",
    "    \"\"\"\n",
    "    grid = np.zeros(shape=(grid_size, grid_size))\n",
    "    algo = BetterUnionFind(grid_size * grid_size + 2)  # rember the virtual nodes\n",
    "\n",
    "    entry_node = grid_size * grid_size\n",
    "    exit_node = grid_size * grid_size + 1\n",
    "\n",
    "    # Connect virtual top to first row and virtual bottom to last row\n",
    "    for col_idx in range(grid_size):\n",
    "        algo.union(entry_node, translate_index(0, col_idx, grid_size))\n",
    "        algo.union(exit_node, translate_index(grid_size - 1, col_idx, grid_size))\n",
    "\n",
    "    ops = [(0, 1), (0, -1), (-1, 0), (1, 0)]\n",
    "    while not algo.connected(entry_node, exit_node):\n",
    "        while True:\n",
    "            idx = rng.integers(0, grid_size)\n",
    "            idy = rng.integers(0, grid_size)\n",
    "            if not grid[idx, idy]:\n",
    "                break\n",
    "\n",
    "        grid[idx, idy] = True\n",
    "        grid_idx = translate_index(idx, idy, grid_size)\n",
    "\n",
    "        for cx, cy in ops:\n",
    "            idcx = idx + cx\n",
    "            idcy = idy + cy\n",
    "            # only take valid and open neighbours\n",
    "            if (0 <= idcx < grid_size) and (0 <= idcy < grid_size) and grid[idcx, idcy]:\n",
    "                neighbor_idx = translate_index(idcx, idcy, grid_size)\n",
    "                algo.union(grid_idx, neighbor_idx)\n",
    "\n",
    "    return np.sum(grid) / grid_size**2\n",
    "\n",
    "\n",
    "def monte_carlo_simulation(num_trials: int) -> dict[int, list[float]]:\n",
    "    \"\"\"Execute the Monte Carlo simulation.\n",
    "\n",
    "    Args:\n",
    "        num_trials: amount of runs for a single configuration.\n",
    "\n",
    "    Returns:\n",
    "        Dict containing the history (trials) for each configuration.\n",
    "\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for grid_size in [5, 25, 50, 100]:\n",
    "        results[grid_size] = [\n",
    "            simulate_experiment(grid_size=grid_size) for _ in range(num_trials)\n",
    "        ]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_percolation_probability_curve(\n",
    "    results: dict[int, list[float]],\n",
    ") -> None:\n",
    "    \"\"\"Plot percolation probability vs site occupation probability.\n",
    "\n",
    "    Args:\n",
    "        results: dictionary containing the histories for each grid size.\n",
    "        num_thresholds: number of probability thresholds to check.\n",
    "\n",
    "    \"\"\"\n",
    "    probability_thresholds = np.linspace(0, 1, 100)\n",
    "\n",
    "    percolation_probabilities = {}\n",
    "    for size, thresholds in results.items():\n",
    "        percolation_probabilities[size] = []\n",
    "\n",
    "        for p in probability_thresholds:\n",
    "            # Calculate the probability of percolation by counting how many\n",
    "            # trials had a threshold less than or equal to p\n",
    "            num_percolating = sum(1 for t in thresholds if t <= p)\n",
    "            probability = num_percolating / len(thresholds)\n",
    "            percolation_probabilities[size].append(probability)\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "    theoretical_threshold = 0.592746  # Theoretical threshold for 2D site percolation\n",
    "\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(results)))\n",
    "\n",
    "    for i, size in enumerate(results):\n",
    "        ax.plot(\n",
    "            probability_thresholds,\n",
    "            percolation_probabilities[size],\n",
    "            \"o-\",\n",
    "            color=colors[i],\n",
    "            label=f\"L = {size}\",\n",
    "            markersize=4,\n",
    "            linewidth=2,\n",
    "            alpha=0.8,\n",
    "        )\n",
    "\n",
    "    ax.axvline(\n",
    "        x=theoretical_threshold,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Theoretical threshold ≈ {theoretical_threshold:.4f}\",\n",
    "    )\n",
    "\n",
    "    ax.axhline(y=0, color=\"gray\", linestyle=\"-\", alpha=0.3)\n",
    "    ax.axhline(y=1, color=\"gray\", linestyle=\"-\", alpha=0.3)\n",
    "\n",
    "    ax.set_xlabel(\"Site Occupation Probability (p)\", fontsize=12)\n",
    "    ax.set_ylabel(\"Percolation Probability\", fontsize=12)\n",
    "    ax.set_title(\"Percolation Probability vs. Site Occupation Probability\", fontsize=14)\n",
    "    ax.grid(visible=True, alpha=0.3)\n",
    "    ax.legend(fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"percolation_probabilities.png\", dpi=300, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = monte_carlo_simulation(100)\n",
    "plot_percolation_probability_curve(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
